{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Contextual Optimization\n\n\nContextual optimization algorithms optimize\n\n\\begin{align}\\max_{\\omega} \\int\\int \\mathcal{R(\\theta, s)}\\pi_{\\omega}(\\theta|s) p(s) ds d\\theta\\end{align}\n\nConventual optimization algorithms optimize just a single scalar, contextual\noptimization optimizes a parametric function or conditional probability\ndistribution. Two instances of contextual optimization algorithms are\nContextual Relative Entropy Policy Search (C-REPS) and Contextual Covariance\nMatrix Adaptation Evolution Strategy (C-CMA-ES). Both use linear models with\nquadratic features and Gaussian exploration as a search distribution.\nDifferences are in the update of the covariance matrix. In this example, we\nuse a very simple objective function to illustrate samples from the search\ndistribution and the current mean of the search distribution. Each sample is\nweighted based on the reward to update the search distribution. Weights are\nindicated by the color of displayed samples.\n\nWe initialize the search distribution with an intentionally low variance.\nC-CMA-ES adapts quite fast, C-REPS is slower because it bounds the\nKullback-Leibler divergence between successive search distributions.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nfrom bolero.optimizer import CCMAESOptimizer, CREPSOptimizer\n\n\ndef objective(x, s):\n    x_offset = x + s.dot(np.array([[0.2]])).dot(s)\n    return -np.array([x_offset.dot(x_offset)])\n\n\ndef plot_objective():\n    contexts, params = np.meshgrid(np.arange(-6, 6, 0.1), np.arange(-6, 6, 0.1))\n    rewards = np.array([[\n        objective(np.array([params[i, j]]), np.array([contexts[i, j]]))[0]\n        for i in range(contexts.shape[0])] for j in range(contexts.shape[1])])\n    plt.contourf(params, contexts, rewards, cmap=plt.cm.Blues,\n                 levels=np.linspace(rewards.min(), rewards.max(), 50))\n    plt.setp(plt.gca(), xticks=(), yticks=(), xlim=(-5, 5), ylim=(-5, 5))\n\n\nrandom_state = np.random.RandomState(0)\ninitial_params = 4.0 * np.ones(1)\nn_samples_per_update = 30\nvariance = 0.03\ncontext_features = \"quadratic\"\nccmaes = CCMAESOptimizer(\n    initial_params=initial_params, n_samples_per_update=n_samples_per_update,\n    variance=variance, context_features=context_features, random_state=0)\ncreps = CREPSOptimizer(\n    initial_params=initial_params, n_samples_per_update=n_samples_per_update,\n    train_freq=n_samples_per_update, variance=variance, epsilon=2.0,\n    context_features=context_features, random_state=0)\nopts = {\"C-CMA-ES\": ccmaes, \"C-REPS\": creps}\nfor opt in opts.values():\n    opt.init(1, 1)\nn_generations = 16\nn_rows = 4\n\nparams = np.empty(1)\nrewards = dict([(k, []) for k in opts.keys()])\ntest_contexts = np.arange(-6, 6, 0.1)\ncolors = {\"C-CMA-ES\": \"r\", \"C-REPS\": \"g\"}\nplt.figure(figsize=(n_generations * 3 / n_rows, 3 * n_rows))\nfor it in range(n_generations):\n    plt.subplot(n_rows, n_generations / n_rows, it + 1)\n    plot_objective()\n\n    contexts = random_state.rand(n_samples_per_update, 1) * 10.0 - 5.0\n    for opt_name, opt in opts.items():\n        last_policy = opt.best_policy()\n        test_last_params = np.array([last_policy(np.array([s]), explore=False)\n                                     for s in test_contexts])\n\n        for i in range(n_samples_per_update):\n            opt.set_context(contexts[i])\n            opt.get_next_parameters(params)\n            f = objective(params, contexts[i])\n            opt.set_evaluation_feedback(f)\n\n        policy = opt.best_policy()\n        test_params = np.array([policy(np.array([s]), explore=False)\n                                for s in test_contexts])\n        mean_reward = np.mean(\n            np.array([objective(p, np.array([s]))[0]\n                      for p, s in zip(test_params, test_contexts)]))\n        rewards[opt_name].append(mean_reward)\n        plt.plot(test_contexts, test_last_params, alpha=0.5,\n                 color=colors[opt_name])\n        plt.plot(test_contexts, test_params, label=opt_name + \" estimate\",\n                 color=colors[opt_name])\n\n        weights = opt.weights\n        if opt_name == \"C-CMA-ES\":\n            marker = \"_\"\n        else:\n            marker = \"|\"\n            weights = opt.weights / np.sum(opt.weights)\n        plt.scatter(contexts.ravel(), np.array(opt.history_theta).ravel(),\n                    c=weights, cmap=plt.cm.gray, marker=marker, s=100,\n                    label=opt_name + \" samples\")\n        if it == 0:\n            plt.legend(loc=\"lower left\")\nplt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n\nplt.figure()\nfor opt_name, values in rewards.items():\n    plt.plot(values, label=opt_name, color=colors[opt_name])\nplt.legend(loc=\"lower right\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"Mean reward\")\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.6", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}